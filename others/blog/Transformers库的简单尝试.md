# Transformersåº“çš„ç®€å•å°è¯•: ä»BERTçš„å¾®è°ƒåˆ°æ›´ä¼˜è´¨çš„Sentence Embedding

> Hugging FaceğŸ¤—å…¬å¸çš„å¼€æºåº“Transformersæ˜¯æ·±åº¦å­¦ä¹ é¢†åŸŸéå¸¸çƒ­é—¨çš„å·¥å…·ï¼Œè¿™ä¸ªåº“é‡Œé¢é›†æˆäº†å„ç§å„æ ·çš„Transformeræ¶æ„çš„æ¨¡å‹ï¼ŒåŒ…æ‹¬BERTç­‰ç­‰ï¼Œå®ƒæä¾›äº†éå¸¸æ–¹ä¾¿çš„APIæ¥æ­å»ºTransformeræ¶æ„çš„æ¨¡å‹æˆ–è€…ä½¿ç”¨é¢„è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œå„ç§ä»»åŠ¡ï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬ä¸»è¦æ¥å°è¯•ä¸€ä¸‹ç”¨åº“ä¸­æä¾›çš„BERTæ¨¡å‹è¿›è¡Œå¥å­åˆ†ç±»ä»»åŠ¡çš„å¾®è°ƒï¼Œå¹¶å°è¯•ä½¿ç”¨SimCSEè¿™æ ·æ›´å¥½çš„Sentence Embeddingæ¥å®Œæˆå¥å­åˆ†ç±»ä»»åŠ¡ã€‚

## Transformersåº“çš„Pipeline

æˆ‘ä»¬é¦–å…ˆè¦äº†è§£ä¸€ä¸‹Transformersè¿™ä¸ªåº“çš„ä½¿ç”¨æ–¹æ³•ï¼Œæˆ‘ä»¬çŸ¥é“PyTorchä½¿ç”¨çš„æ—¶å€™æœ‰è¿™æ ·å‡ ä¸ªä¸œè¥¿éœ€è¦å®šä¹‰ï¼š

- æ•°æ®é›†çš„`Dataset`ç±»å’Œæ•°æ®åŠ è½½å™¨`DataLoader`
- æ¨¡å‹æœ¬èº«çš„ä»£ç ï¼Œéœ€è¦ç»§æ‰¿`nn.Module`ç±»
- æ¨¡å‹çš„è®­ç»ƒï¼Œæµ‹è¯•ç­‰è¿‡ç¨‹çš„ä»£ç 

è€ŒTransformeråº“çš„ä½¿ç”¨è¿‡ç¨‹ä¸­ä¾ç„¶éœ€è¦éµå¾ªè¿™ä¸ªåŸºæœ¬çš„æ¨¡å¼ï¼Œä½†æ˜¯åœ¨è°ƒç”¨å®ƒæä¾›çš„é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œéœ€è¦å¦å¤–è€ƒè™‘è¿™æ ·å‡ ä¸ªä¸œè¥¿(æˆ‘ä»¬ä»¥BERTæ¨¡å‹ä¸ºä¾‹)ï¼š

- åˆ†è¯å™¨Tokenizerï¼Œå°†Raw Dataå¤„ç†æˆä¸€ä¸ªtokenåºåˆ—è¾“å…¥æ¨¡å‹ä¸­
- æ¨¡å‹çš„é…ç½®ä¿¡æ¯BertConfigï¼Œè¿™ä¸ªç±»è®°å½•äº†ä¸€ä¸ªBERTæ¨¡å‹çš„é…ç½®ä¿¡æ¯
- æ¨¡å‹BertModelï¼Œè¿™ä¸ªå°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„BERTæ¨¡å‹æ¶æ„ï¼Œå¯ä»¥ç”¨BertConfigæ¥è¿›è¡Œåˆå§‹åŒ–
- åŒæ—¶Transformersåº“ä¸­å¯ä»¥ç”¨`from_pretrained()` æ–¹æ³•åœ¨åŠ è½½é¢„è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°ï¼Œä¹Ÿå¯ä»¥åˆå§‹åŒ–åƒåˆ†è¯å™¨ä¹‹ç±»çš„é‡è¦ç»„ä»¶

ä½¿ç”¨Transformersåº“è¿›è¡Œæœºå™¨å­¦ä¹ ä»»åŠ¡çš„æ•´ä¸ªPipelineå’ŒPyTorchåŸºæœ¬æ˜¯ä¸€è‡´çš„ï¼Œåªä¸è¿‡å®ƒé¢å¤–æä¾›äº†ä¸€äº›å¾ˆæ–¹ä¾¿çš„APIæ¥è°ƒç”¨åŒ…æ‹¬BERTåœ¨å†…çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚



## ç®€å•çš„BERTæ–‡æœ¬åˆ†ç±»

æˆ‘ä»¬å°è¯•ç”¨BERTè¿›è¡Œç®€å•çš„æ–‡æœ¬åˆ†ç±»ï¼Œè¿™ä¸€æ¬¡æˆ‘ä»¬ä½¿ç”¨çš„æ•°æ®é›†æ˜¯æ–¯å¦ç¦å¤§å­¦ç ”ç©¶è€…æå‡ºçš„SST-1æ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ª**å¥å­çº§çš„æƒ…æ„Ÿåˆ†ç±»**æ•°æ®é›†ï¼Œä¸€å…±æœ‰5ç§æƒ…æ„Ÿç±»åˆ«ï¼Œå®ƒçš„è®­ç»ƒé›†ä¸€å…±æœ‰8544æ¡æ•°æ®ï¼Œè€Œæµ‹è¯•é›†æœ‰2210æ¡æ•°æ®ï¼Œè¿™äº›æ•°æ®éƒ½æ˜¯æ¯”è¾ƒçŸ­çš„å¥å­ï¼Œå› æ­¤SST-1æ˜¯ä¸€ä¸ªæ¯”è¾ƒå°çš„æ•°æ®é›†ã€‚(BERTåŸè®ºæ–‡é‡Œçš„SST-2å’ŒSST-1ç±»ä¼¼ï¼Œä½†æ˜¯SST-2æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»æ•°æ®é›†ï¼Œç›¸æ¯”ä¹‹ä¸‹SST-1çš„ç²’åº¦æ›´ç»†ä¸€ç‚¹)

![image-20220220202026697](static/image-20220220202026697.png)

ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨BERTæ¥è¿›è¡Œå¾®è°ƒå®éªŒï¼Œä½¿ç”¨çš„æ¨¡å‹ä¹Ÿéå¸¸ç®€å•ï¼Œå°±æ˜¯å°†è¾“å…¥çš„å¥å­é€šè¿‡BERTç¼–ç ä¹‹åï¼Œç”¨ä¸€å®šçš„æ–¹å¼å¾—åˆ°æ•´ä¸ªå¥å­çš„è¡¨ç¤ºï¼Œç„¶åé€šè¿‡ä¸€ä¸ª**çº¿æ€§å±‚æŠ•å½±**åˆ°5ç§åˆ†ç±»ç»“æœä¸­ï¼Œå¹¶é€šè¿‡SoftMaxæ¥è®¡ç®—è¯¥å¥å­**å±äºæ¯ä¸€ç±»åˆ«çš„æ¦‚ç‡**ï¼Œç„¶åç”¨äº¤å‰ç†µä½œä¸ºlosså‡½æ•°å³å¯ï¼Œè¿™é‡Œå¾—åˆ°å¥å­çš„ç¼–ç è¡¨ç¤ºçš„æ–¹æ³•æœ‰ä¸¤ç§ï¼Œåˆ†åˆ«æ˜¯ï¼š

- ç›´æ¥ä½¿ç”¨å¥å­å¤´éƒ¨çš„**CLSæ ‡ç­¾**ä½œä¸ºæ•´ä¸ªå¥å­çš„ç¼–ç è¡¨ç¤º
- ä½¿ç”¨æ•´ä¸ªå¥å­æ‰€æœ‰tokençš„è¡¨ç¤ºçš„**å¹³å‡å€¼**ä½œä¸ºå¥å­çš„ç¼–ç è¡¨ç¤º

è€Œæˆ‘ä»¬ä½¿ç”¨çš„BERTæ¨¡å‹ä¹Ÿå¯ä»¥æ›´æ¢æˆå…¶ä»–çš„ï¼Œæ¯”å¦‚åé¢ä¼šæåˆ°çš„ä½¿ç”¨BERTç›¸åŒæ¶æ„ä½†æ˜¯é¢„è®­ç»ƒç›®æ ‡ä¸åŒçš„SimCSEæ¨¡å‹ã€‚

å‰©ä¸‹çš„äº‹æƒ…å°±åªå‰©ä¸‹codingäº†ï¼Œå…¶ä¸­æ¨¡å‹éƒ¨åˆ†çš„ä»£ç å¦‚ä¸‹ï¼š

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import BertModel, AutoModel

class BertForTextClassification(nn.Module):
    def __init__(self, bert_config, num_labels, mode='cls', model_type='bert') -> None:
        super(BertForTextClassification, self).__init__()
        self.type = model_type
        if model_type == 'bert':
            self.bert = BertModel.from_pretrained("bert-base-uncased")
        else:
            self.bert = AutoModel.from_pretrained('princeton-nlp/sup-simcse-bert-base-uncased')
        self.proj = nn.Linear(bert_config.hidden_size, num_labels)
        if mode not in ['cls', 'pooling']:
            raise NotImplementedError
        self.mode = mode
    
    def forward(self, input_ids, attention_mask, token_type_ids):
        if self.type == 'bert':
            bert_output = self.bert(input_ids, attention_mask, token_type_ids).last_hidden_state
            if self.mode == 'cls':
                bert_output = bert_output[:, 0, :]
            elif self.mode == 'pooling':
                bert_output = torch.mean(bert_output, dim=1)
        else:
            bert_output = self.bert(input_ids, attention_mask, token_type_ids, output_hidden_states=True, return_dict=True).pooler_output
        proj_output = self.proj(bert_output)
        return proj_output

```

ä¹‹åå°±å¯ä»¥å¼€å§‹è®­ç»ƒï¼Œå¯¹ä¸¤ç§ä¸åŒçš„å¥å­è¡¨ç¤ºæ–¹æ³•ï¼Œæˆ‘ä»¬éƒ½ä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ï¼Œå¹¶ä½¿ç”¨Adamä½œä¸ºä¼˜åŒ–å™¨ï¼Œè¶…å‚æ•°è®¾ç½®å¦‚ä¸‹ï¼š

| å‚æ•°          | è®¾å®šå€¼ |
| ------------- | ------ |
| batch size    | 32     |
| learning rate | 2e-5   |

å¾—åˆ°çš„ç»“æœå¦‚ä¸‹ï¼š

| æ¨¡å‹              | åœ¨æµ‹è¯•é›†ä¸Šçš„å‡†ç¡®ç‡(%) |
| ----------------- | --------------------- |
| BERT+CLS          | 52.03                 |
| BERT+Mean Pooling | 51.04                 |

æˆ‘ä»¬å‘ç°åœ¨è¿™ä¸ªä»»åŠ¡ä¸Šï¼ŒCLSæ ‡ç­¾ä½œä¸ºå¥å­è¡¨ç¤ºçš„æ•ˆæœæ›´å¥½ï¼ŒåŒæ—¶å¯¹æ¯”ä¸€äº›ç°æœ‰çš„baselineï¼Œæ ¹æ®æˆ‘æŸ¥åˆ°çš„æ•°æ®ï¼Œä¼ ç»Ÿçš„åŸºäºè¯å‘é‡ä»¥åŠTextCNNçš„å„ç§æ–¹æ³•åœ¨SST-1ä»»åŠ¡ä¸Šçš„è¡¨ç°ä¸€èˆ¬éƒ½åœ¨45%-50%ä¹‹é—´ï¼Œå¯ä»¥çœ‹å‡ºBERTç›¸æ¯”äºè¿™äº›ä¼ ç»Ÿçš„æ–‡æœ¬åˆ†ç±»æ–¹æ³•è¦å¥½å¾ˆå¤šã€‚



## SimCSE: æ›´å¥½çš„Sentence Embedding

SimCSEæ˜¯åœ¨è®ºæ–‡ã€Š[SimCSE: Simple Contrastive Learning of Sentence Embeddings](https://arxiv.org/pdf/2104.08821.pdf)ã€‹è¿™ç¯‡è®ºæ–‡ç§æå‡ºçš„ï¼Œå®ƒè¿˜æ˜¯é‡‡ç”¨äº†BERTçš„æ¶æ„ï¼Œä½†æ˜¯æ”¹å˜äº†é¢„è®­ç»ƒä»»åŠ¡ï¼Œè®©æ¨¡å‹èƒ½ç”Ÿæˆæ›´åˆé€‚çš„å¥å­çº§åˆ«çš„è¡¨ç¤º(BERTçš„é¢„è®­ç»ƒä»»åŠ¡æ˜¯è¯çº§åˆ«çš„MLMï¼Œæ‰€ä»¥å®ƒçš„è¯è¡¨ç¤ºä¼¼ä¹æ›´å¥½ä¸€ç‚¹)

![image-20220220203753810](static/image-20220220203753810.png)

æˆ‘ä»¬ç”¨é¢„è®­ç»ƒå¥½çš„SimCSEæ¨¡å‹æ¥è¿›è¡Œå’Œä¸Šé¢ä¸€æ ·çš„å¾®è°ƒï¼Œå¾—åˆ°åœ¨SST-1çš„æµ‹è¯•é›†ä¸Šçš„é¢„æµ‹å‡†ç¡®ç‡çº¦ä¸º53.53%ï¼Œè¿™è¯´æ˜SimCSEåœ¨æƒ…æ„Ÿåˆ†ç±»è¿™æ ·å¥å­çº§åˆ«çš„ä»»åŠ¡ä¸Šï¼Œç¡®å®è¦æ¯”BERT-baseè¡¨ç°å¾—æ›´å¥½ã€‚